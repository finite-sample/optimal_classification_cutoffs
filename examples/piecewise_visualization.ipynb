{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Visualization of Piecewise-Constant Metrics\n",
    "\n",
    "This notebook demonstrates why classification metrics like F1 score are piecewise-constant functions and illustrates the challenges this creates for continuous optimization methods.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Piecewise-constant**: Metrics only change at unique probability values\n",
    "- **Breakpoints**: The unique predicted probabilities where metrics can change\n",
    "- **Flat regions**: Intervals between breakpoints where the metric stays constant\n",
    "- **Optimization challenge**: Continuous optimizers may miss the global optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import optimal_cutoffs functions\n",
    "import sys\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from scipy import optimize\n",
    "\n",
    "sys.path.append('..')\n",
    "from optimal_cutoffs.optimizers import _metric_score\n",
    "\n",
    "from optimal_cutoffs import get_confusion_matrix, get_optimal_threshold\n",
    "\n",
    "# Set up matplotlib for interactive plots\n",
    "%matplotlib widget\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Demonstration\n",
    "\n",
    "Let's start with a simple example to see the piecewise-constant nature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "y_true = np.array([0, 0, 1, 1, 0, 1, 0])\n",
    "y_prob = np.array([0.1, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "print(\"Example data:\")\n",
    "print(f\"True labels:  {y_true}\")\n",
    "print(f\"Probabilities: {y_prob}\")\n",
    "print(f\"\\nUnique probabilities (breakpoints): {np.unique(y_prob)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_piecewise_metric(y_true, y_prob, metric='f1', title_suffix=''):\n",
    "    \"\"\"Plot a metric vs threshold showing piecewise-constant behavior.\"\"\"\n",
    "\n",
    "    # Generate dense threshold grid for smooth plotting\n",
    "    thresholds = np.linspace(0.05, 0.95, 500)\n",
    "    scores = [_metric_score(y_true, y_prob, t, metric) for t in thresholds]\n",
    "\n",
    "    # Find breakpoints (unique probabilities)\n",
    "    breakpoints = np.unique(y_prob)\n",
    "    breakpoint_scores = [_metric_score(y_true, y_prob, t, metric) for t in breakpoints]\n",
    "\n",
    "    # Find optimal threshold\n",
    "    optimal_threshold = get_optimal_threshold(y_true, y_prob, metric, method='smart_brute')\n",
    "    optimal_score = _metric_score(y_true, y_prob, optimal_threshold, metric)\n",
    "\n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "    # Plot the metric function\n",
    "    ax.plot(thresholds, scores, 'b-', linewidth=2, label=f'{metric.upper()} Score')\n",
    "\n",
    "    # Mark breakpoints\n",
    "    ax.scatter(breakpoints, breakpoint_scores, color='red', s=80, zorder=5,\n",
    "              label=f'Breakpoints ({len(breakpoints)} points)')\n",
    "\n",
    "    # Mark optimal\n",
    "    ax.scatter([optimal_threshold], [optimal_score], color='green', s=150,\n",
    "              marker='*', zorder=6, label=f'Optimal (t={optimal_threshold:.3f})')\n",
    "\n",
    "    # Add vertical lines at breakpoints\n",
    "    for bp in breakpoints:\n",
    "        ax.axvline(x=bp, color='red', linestyle='--', alpha=0.3)\n",
    "\n",
    "    ax.set_xlabel('Decision Threshold')\n",
    "    ax.set_ylabel(f'{metric.upper()} Score')\n",
    "    ax.set_title(f'Piecewise-Constant Nature of {metric.upper()} Score{title_suffix}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1.05)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return fig, optimal_threshold, optimal_score\n",
    "\n",
    "# Plot F1 score for our example\n",
    "fig, opt_thresh, opt_score = plot_piecewise_metric(y_true, y_prob, 'f1')\n",
    "print(f\"\\nOptimal F1 threshold: {opt_thresh:.3f} (F1 = {opt_score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Interactive Exploration\n",
    "\n",
    "Use the sliders below to see how changing the data affects the piecewise-constant structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_demo():\n",
    "    \"\"\"Create an interactive widget for exploring piecewise-constant behavior.\"\"\"\n",
    "\n",
    "    # Create sliders for data generation\n",
    "    n_samples_slider = widgets.IntSlider(\n",
    "        value=10, min=5, max=20, step=1,\n",
    "        description='N Samples:'\n",
    "    )\n",
    "\n",
    "    pos_ratio_slider = widgets.FloatSlider(\n",
    "        value=0.5, min=0.1, max=0.9, step=0.1,\n",
    "        description='Pos Ratio:'\n",
    "    )\n",
    "\n",
    "    seed_slider = widgets.IntSlider(\n",
    "        value=42, min=0, max=100, step=1,\n",
    "        description='Random Seed:'\n",
    "    )\n",
    "\n",
    "    metric_dropdown = widgets.Dropdown(\n",
    "        options=['f1', 'accuracy', 'precision', 'recall'],\n",
    "        value='f1',\n",
    "        description='Metric:'\n",
    "    )\n",
    "\n",
    "    def update_plot(n_samples, pos_ratio, seed, metric):\n",
    "        # Generate random data\n",
    "        np.random.seed(seed)\n",
    "        n_pos = int(n_samples * pos_ratio)\n",
    "        n_neg = n_samples - n_pos\n",
    "\n",
    "        y_true = np.concatenate([np.zeros(n_neg), np.ones(n_pos)])\n",
    "        y_prob = np.random.beta(2, 2, n_samples)  # Bell-shaped distribution\n",
    "\n",
    "        # Sort by probability for cleaner visualization\n",
    "        sort_idx = np.argsort(y_prob)\n",
    "        y_true = y_true[sort_idx]\n",
    "        y_prob = y_prob[sort_idx]\n",
    "\n",
    "        # Plot\n",
    "        plt.clf()\n",
    "        fig, opt_thresh, opt_score = plot_piecewise_metric(\n",
    "            y_true, y_prob, metric,\n",
    "            title_suffix=f'\\n{n_samples} samples, {len(np.unique(y_prob))} unique probabilities'\n",
    "        )\n",
    "\n",
    "        print(f\"Generated {n_samples} samples ({n_pos} positive, {n_neg} negative)\")\n",
    "        print(f\"Optimal {metric} threshold: {opt_thresh:.3f} (score = {opt_score:.3f})\")\n",
    "        print(f\"Number of breakpoints: {len(np.unique(y_prob))}\")\n",
    "\n",
    "    # Create interactive widget\n",
    "    interactive_plot = widgets.interactive(\n",
    "        update_plot,\n",
    "        n_samples=n_samples_slider,\n",
    "        pos_ratio=pos_ratio_slider,\n",
    "        seed=seed_slider,\n",
    "        metric=metric_dropdown\n",
    "    )\n",
    "\n",
    "    display(interactive_plot)\n",
    "\n",
    "create_interactive_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimization Methods Comparison\n",
    "\n",
    "Let's compare different optimization approaches on the same data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_optimization_methods(y_true, y_prob, metric='f1'):\n",
    "    \"\"\"Compare different threshold optimization methods.\"\"\"\n",
    "\n",
    "    print(f\"Comparing optimization methods for {metric.upper()} score...\\n\")\n",
    "\n",
    "    # Method 1: Smart brute force (our recommended approach)\n",
    "    thresh_brute = get_optimal_threshold(y_true, y_prob, metric, method='smart_brute')\n",
    "    score_brute = _metric_score(y_true, y_prob, thresh_brute, metric)\n",
    "\n",
    "    # Method 2: scipy.optimize.minimize_scalar (continuous optimization)\n",
    "    result = optimize.minimize_scalar(\n",
    "        lambda t: -_metric_score(y_true, y_prob, t, metric),\n",
    "        bounds=(0, 1),\n",
    "        method='bounded'\n",
    "    )\n",
    "    thresh_minimize = result.x\n",
    "    score_minimize = _metric_score(y_true, y_prob, thresh_minimize, metric)\n",
    "\n",
    "    # Method 3: With fallback (what our 'minimize' method actually does)\n",
    "    thresh_fallback = get_optimal_threshold(y_true, y_prob, metric, method='minimize')\n",
    "    score_fallback = _metric_score(y_true, y_prob, thresh_fallback, metric)\n",
    "\n",
    "    # Display results\n",
    "    methods = [\n",
    "        ('Smart Brute Force', thresh_brute, score_brute),\n",
    "        ('minimize_scalar Only', thresh_minimize, score_minimize),\n",
    "        ('With Fallback', thresh_fallback, score_fallback)\n",
    "    ]\n",
    "\n",
    "    for name, threshold, score in methods:\n",
    "        print(f\"{name:18} | Threshold: {threshold:.4f} | {metric.upper()}: {score:.4f}\")\n",
    "\n",
    "    # Create visualization\n",
    "    thresholds = np.linspace(0.01, 0.99, 500)\n",
    "    scores = [_metric_score(y_true, y_prob, t, metric) for t in thresholds]\n",
    "\n",
    "    unique_probs = np.unique(y_prob)\n",
    "    unique_scores = [_metric_score(y_true, y_prob, t, metric) for t in unique_probs]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "    # Plot metric function\n",
    "    ax.plot(thresholds, scores, 'b-', linewidth=1.5, alpha=0.7, label=f'{metric.upper()} Score')\n",
    "\n",
    "    # Plot breakpoints\n",
    "    ax.scatter(unique_probs, unique_scores, color='lightcoral', s=30, alpha=0.6,\n",
    "              label=f'Breakpoints ({len(unique_probs)} points)')\n",
    "\n",
    "    # Plot results from different methods\n",
    "    colors = ['green', 'red', 'blue']\n",
    "    markers = ['*', 'x', 'D']\n",
    "\n",
    "    for (name, threshold, score), color, marker in zip(methods, colors, markers, strict=False):\n",
    "        ax.scatter([threshold], [score], color=color, s=120, marker=marker,\n",
    "                  zorder=5, label=f'{name}\\n(t={threshold:.3f})', edgecolors='black')\n",
    "\n",
    "    ax.set_xlabel('Decision Threshold')\n",
    "    ax.set_ylabel(f'{metric.upper()} Score')\n",
    "    ax.set_title('Comparison of Optimization Methods')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return methods\n",
    "\n",
    "# Test with example data\n",
    "np.random.seed(123)\n",
    "n = 15\n",
    "y_test = np.random.randint(0, 2, n)\n",
    "y_prob_test = np.random.beta(2, 2, n)\n",
    "\n",
    "print(f\"Test data: {n} samples with {len(np.unique(y_prob_test))} unique probabilities\\n\")\n",
    "results = compare_optimization_methods(y_test, y_prob_test, 'f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Why the Fallback Mechanism Works\n",
    "\n",
    "The key insight is that **the optimal threshold must be one of the unique predicted probabilities**. Here's why:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_optimal_at_breakpoints():\n",
    "    \"\"\"Show that the optimal threshold is always at a breakpoint.\"\"\"\n",
    "\n",
    "    # Create example with clear optimal point\n",
    "    y_true = np.array([0, 0, 1, 1, 0, 1])\n",
    "    y_prob = np.array([0.2, 0.3, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "    print(\"Demonstrating that optimal threshold is at a breakpoint...\\n\")\n",
    "    print(f\"Data: labels = {y_true}\")\n",
    "    print(f\"      probs  = {y_prob}\\n\")\n",
    "\n",
    "    # Evaluate F1 at each unique probability\n",
    "    unique_probs = np.unique(y_prob)\n",
    "    print(\"F1 score at each unique probability (breakpoint):\")\n",
    "\n",
    "    for _i, prob in enumerate(unique_probs):\n",
    "        f1 = _metric_score(y_true, y_prob, prob, 'f1')\n",
    "        tp, tn, fp, fn = get_confusion_matrix(y_true, y_prob, prob)\n",
    "        print(f\"  t = {prob:.1f}: F1 = {f1:.3f} | TP={tp}, TN={tn}, FP={fp}, FN={fn}\")\n",
    "\n",
    "    # Find optimal\n",
    "    optimal_thresh = get_optimal_threshold(y_true, y_prob, 'f1')\n",
    "    optimal_f1 = _metric_score(y_true, y_prob, optimal_thresh, 'f1')\n",
    "\n",
    "    print(f\"\\n→ Optimal: t = {optimal_thresh:.1f}, F1 = {optimal_f1:.3f}\")\n",
    "\n",
    "    # Now test a threshold between breakpoints\n",
    "    between_thresh = 0.65  # Between 0.6 and 0.7\n",
    "    between_f1 = _metric_score(y_true, y_prob, between_thresh, 'f1')\n",
    "\n",
    "    print(f\"\\nFor comparison, at t = {between_thresh:.2f} (between breakpoints):\")\n",
    "    print(f\"  F1 = {between_f1:.3f} (same as t = 0.6 because both give same predictions)\")\n",
    "\n",
    "    # Visualize predictions at different thresholds\n",
    "    print(\"\\nPrediction vectors:\")\n",
    "    for thresh in [0.6, 0.65, 0.7]:\n",
    "        predictions = (y_prob >= thresh).astype(int)\n",
    "        print(f\"  t = {thresh:.2f}: {predictions}\")\n",
    "\n",
    "    print(\"\\n→ Note: t=0.6 and t=0.65 give the same predictions, hence same F1!\")\n",
    "\n",
    "demonstrate_optimal_at_breakpoints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multiple Metrics Comparison\n",
    "\n",
    "Different metrics often have different optimal thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_multiple_metrics(y_true, y_prob):\n",
    "    \"\"\"Show how different metrics have different optimal thresholds.\"\"\"\n",
    "\n",
    "    metrics = ['accuracy', 'f1', 'precision', 'recall']\n",
    "    colors = ['blue', 'red', 'green', 'orange']\n",
    "\n",
    "    thresholds = np.linspace(0.05, 0.95, 200)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for metric, color in zip(metrics, colors, strict=False):\n",
    "        # Calculate scores across threshold range\n",
    "        scores = [_metric_score(y_true, y_prob, t, metric) for t in thresholds]\n",
    "        ax.plot(thresholds, scores, color=color, linewidth=2, label=metric.capitalize())\n",
    "\n",
    "        # Find optimal threshold\n",
    "        optimal_thresh = get_optimal_threshold(y_true, y_prob, metric)\n",
    "        optimal_score = _metric_score(y_true, y_prob, optimal_thresh, metric)\n",
    "\n",
    "        # Mark optimal point\n",
    "        ax.scatter([optimal_thresh], [optimal_score], color=color, s=150,\n",
    "                  marker='*', zorder=5, edgecolors='black', linewidth=1)\n",
    "\n",
    "        results[metric] = (optimal_thresh, optimal_score)\n",
    "\n",
    "    # Add breakpoint lines\n",
    "    unique_probs = np.unique(y_prob)\n",
    "    for prob in unique_probs:\n",
    "        ax.axvline(x=prob, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "    ax.set_xlabel('Decision Threshold')\n",
    "    ax.set_ylabel('Metric Score')\n",
    "    ax.set_title('Different Metrics Have Different Optimal Thresholds\\n' +\n",
    "                '(Stars show optimal points, dashed lines show breakpoints)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1.05)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print results\n",
    "    print(\"Optimal thresholds by metric:\")\n",
    "    for metric, (thresh, score) in results.items():\n",
    "        print(f\"  {metric:9}: t = {thresh:.3f}, score = {score:.3f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Demo with well-separated data\n",
    "y_demo = np.array([0, 0, 0, 1, 1, 1])\n",
    "p_demo = np.array([0.1, 0.3, 0.4, 0.6, 0.8, 0.9])\n",
    "\n",
    "print(f\"Demo data: labels = {y_demo}\")\n",
    "print(f\"           probs  = {p_demo}\\n\")\n",
    "\n",
    "metric_results = compare_multiple_metrics(y_demo, p_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practical Implications\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Piecewise-Constant Nature**: Classification metrics only change at unique probability values\n",
    "\n",
    "2. **Optimization Challenge**: Continuous optimizers can get stuck in flat regions and miss the global optimum\n",
    "\n",
    "3. **Smart Solution**: Evaluate metrics at all unique probabilities (guaranteed global optimum)\n",
    "\n",
    "4. **Fallback Mechanism**: Combine continuous optimization with discrete evaluation for robustness\n",
    "\n",
    "5. **Metric Differences**: Different metrics often have different optimal thresholds\n",
    "\n",
    "### When This Matters Most\n",
    "\n",
    "- **Imbalanced datasets**: Default 0.5 threshold is often far from optimal\n",
    "- **Cost-sensitive decisions**: When false positives and false negatives have different costs\n",
    "- **Metric optimization**: When you need to maximize a specific metric (F1, precision, recall)\n",
    "- **Model deployment**: When converting probabilities to hard predictions\n",
    "\n",
    "### Computational Efficiency\n",
    "\n",
    "The smart brute force approach is actually very efficient:\n",
    "- **Time complexity**: O(k) where k = number of unique probabilities\n",
    "- **Typical case**: k ≪ n (much fewer unique probabilities than samples)\n",
    "- **Guaranteed optimum**: No risk of local minima or convergence issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration: efficiency comparison\n",
    "import time\n",
    "\n",
    "\n",
    "def efficiency_demo():\n",
    "    \"\"\"Demonstrate the efficiency of smart brute force vs continuous optimization.\"\"\"\n",
    "\n",
    "    # Generate larger dataset\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    y_large = np.random.randint(0, 2, n_samples)\n",
    "    p_large = np.random.beta(2, 2, n_samples)\n",
    "\n",
    "    n_unique = len(np.unique(p_large))\n",
    "\n",
    "    print(f\"Efficiency test with {n_samples} samples, {n_unique} unique probabilities\\n\")\n",
    "\n",
    "    methods = [\n",
    "        ('smart_brute', 'Smart Brute Force'),\n",
    "        ('minimize', 'Minimize with Fallback'),\n",
    "        ('gradient', 'Gradient Method')\n",
    "    ]\n",
    "\n",
    "    for method_code, method_name in methods:\n",
    "        start_time = time.time()\n",
    "        threshold = get_optimal_threshold(y_large, p_large, 'f1', method=method_code)\n",
    "        end_time = time.time()\n",
    "\n",
    "        score = _metric_score(y_large, p_large, threshold, 'f1')\n",
    "        duration = end_time - start_time\n",
    "\n",
    "        print(f\"{method_name:20} | Time: {duration:.4f}s | F1: {score:.4f} | Threshold: {threshold:.4f}\")\n",
    "\n",
    "    print(f\"\\n→ Smart brute force evaluates only {n_unique} points vs {n_samples} samples!\")\n",
    "\n",
    "efficiency_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the piecewise-constant nature of classification metrics and why this creates challenges for traditional optimization methods. The `optimal-classification-cutoffs` library addresses these challenges through:\n",
    "\n",
    "1. **Smart algorithms** that leverage the mathematical structure of the problem\n",
    "2. **Fallback mechanisms** that ensure robust optimization\n",
    "3. **Efficient implementation** that scales well with dataset size\n",
    "\n",
    "For more details, see the [full documentation](https://finite-sample.github.io/optimal_classification_cutoffs/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}