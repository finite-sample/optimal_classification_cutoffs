{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ðŸ”¬ Interactive Demo: Deep Dive into Threshold Behavior\n**ROI**: Understand why optimal thresholds work and build intuition  \n**Time**: 20+ minutes of exploration and discovery  \n**Previous**: Run 01_quickstart.py â†’ 02_business_value.py â†’ 03_multiclass.py first\n\nThis interactive notebook lets you explore the mathematical foundations behind optimal threshold selection. Perfect for understanding why the library works so well!\n\n## ðŸŽ¯ Key Learning Objectives\n\n- **Piecewise-constant**: Why metrics only change at specific points\n- **Breakpoints**: The unique probabilities where metrics can change  \n- **Optimization challenges**: Why continuous methods can fail\n- **Algorithm insights**: How smart methods guarantee global optimum\n\n## ðŸš€ Quick Start\nRun the cells below to start exploring. Use the interactive widgets to see how different data characteristics affect optimal thresholds."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import optimal_cutoffs functions\nimport sys\n\nimport ipywidgets as widgets\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom IPython.display import display\nfrom scipy import optimize\n\nsys.path.append('..')\nfrom optimal_cutoffs.optimizers import _metric_score\n\nfrom optimal_cutoffs import optimize_thresholds\n\n# Set up matplotlib for interactive plots\n%matplotlib widget\nplt.style.use('default')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Demonstration\n",
    "\n",
    "Let's start with a simple example to see the piecewise-constant nature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "y_true = np.array([0, 0, 1, 1, 0, 1, 0])\n",
    "y_prob = np.array([0.1, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "print(\"Example data:\")\n",
    "print(f\"True labels:  {y_true}\")\n",
    "print(f\"Probabilities: {y_prob}\")\n",
    "print(f\"\\nUnique probabilities (breakpoints): {np.unique(y_prob)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_piecewise_metric(y_true, y_prob, metric='f1', title_suffix=''):\n    \"\"\"Plot a metric vs threshold showing piecewise-constant behavior.\"\"\"\n\n    # Generate dense threshold grid for smooth plotting\n    thresholds = np.linspace(0.05, 0.95, 500)\n    scores = [_metric_score(y_true, y_prob, t, metric) for t in thresholds]\n\n    # Find breakpoints (unique probabilities)\n    breakpoints = np.unique(y_prob)\n    breakpoint_scores = [_metric_score(y_true, y_prob, t, metric) for t in breakpoints]\n\n    # Find optimal threshold\n    result = optimize_thresholds(y_true, y_prob, metric=metric, method='smart_brute')\n    optimal_threshold = result.threshold\n    optimal_score = _metric_score(y_true, y_prob, optimal_threshold, metric)\n\n    # Create plot\n    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n\n    # Plot the metric function\n    ax.plot(thresholds, scores, 'b-', linewidth=2, label=f'{metric.upper()} Score')\n\n    # Mark breakpoints\n    ax.scatter(breakpoints, breakpoint_scores, color='red', s=80, zorder=5,\n              label=f'Breakpoints ({len(breakpoints)} points)')\n\n    # Mark optimal\n    ax.scatter([optimal_threshold], [optimal_score], color='green', s=150,\n              marker='*', zorder=6, label=f'Optimal (t={optimal_threshold:.3f})')\n\n    # Add vertical lines at breakpoints\n    for bp in breakpoints:\n        ax.axvline(x=bp, color='red', linestyle='--', alpha=0.3)\n\n    ax.set_xlabel('Decision Threshold')\n    ax.set_ylabel(f'{metric.upper()} Score')\n    ax.set_title(f'Piecewise-Constant Nature of {metric.upper()} Score{title_suffix}')\n    ax.grid(True, alpha=0.3)\n    ax.legend()\n    ax.set_ylim(0, 1.05)\n\n    plt.tight_layout()\n    plt.show()\n\n    return fig, optimal_threshold, optimal_score\n\n# Plot F1 score for our example\nfig, opt_thresh, opt_score = plot_piecewise_metric(y_true, y_prob, 'f1')\nprint(f\"\\nOptimal F1 threshold: {opt_thresh:.3f} (F1 = {opt_score:.3f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Interactive Exploration\n",
    "\n",
    "Use the sliders below to see how changing the data affects the piecewise-constant structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_demo():\n",
    "    \"\"\"Create an interactive widget for exploring piecewise-constant behavior.\"\"\"\n",
    "\n",
    "    # Create sliders for data generation\n",
    "    n_samples_slider = widgets.IntSlider(\n",
    "        value=10, min=5, max=20, step=1,\n",
    "        description='N Samples:'\n",
    "    )\n",
    "\n",
    "    pos_ratio_slider = widgets.FloatSlider(\n",
    "        value=0.5, min=0.1, max=0.9, step=0.1,\n",
    "        description='Pos Ratio:'\n",
    "    )\n",
    "\n",
    "    seed_slider = widgets.IntSlider(\n",
    "        value=42, min=0, max=100, step=1,\n",
    "        description='Random Seed:'\n",
    "    )\n",
    "\n",
    "    metric_dropdown = widgets.Dropdown(\n",
    "        options=['f1', 'accuracy', 'precision', 'recall'],\n",
    "        value='f1',\n",
    "        description='Metric:'\n",
    "    )\n",
    "\n",
    "    def update_plot(n_samples, pos_ratio, seed, metric):\n",
    "        # Generate random data\n",
    "        np.random.seed(seed)\n",
    "        n_pos = int(n_samples * pos_ratio)\n",
    "        n_neg = n_samples - n_pos\n",
    "\n",
    "        y_true = np.concatenate([np.zeros(n_neg), np.ones(n_pos)])\n",
    "        y_prob = np.random.beta(2, 2, n_samples)  # Bell-shaped distribution\n",
    "\n",
    "        # Sort by probability for cleaner visualization\n",
    "        sort_idx = np.argsort(y_prob)\n",
    "        y_true = y_true[sort_idx]\n",
    "        y_prob = y_prob[sort_idx]\n",
    "\n",
    "        # Plot\n",
    "        plt.clf()\n",
    "        fig, opt_thresh, opt_score = plot_piecewise_metric(\n",
    "            y_true, y_prob, metric,\n",
    "            title_suffix=f'\\n{n_samples} samples, {len(np.unique(y_prob))} unique probabilities'\n",
    "        )\n",
    "\n",
    "        print(f\"Generated {n_samples} samples ({n_pos} positive, {n_neg} negative)\")\n",
    "        print(f\"Optimal {metric} threshold: {opt_thresh:.3f} (score = {opt_score:.3f})\")\n",
    "        print(f\"Number of breakpoints: {len(np.unique(y_prob))}\")\n",
    "\n",
    "    # Create interactive widget\n",
    "    interactive_plot = widgets.interactive(\n",
    "        update_plot,\n",
    "        n_samples=n_samples_slider,\n",
    "        pos_ratio=pos_ratio_slider,\n",
    "        seed=seed_slider,\n",
    "        metric=metric_dropdown\n",
    "    )\n",
    "\n",
    "    display(interactive_plot)\n",
    "\n",
    "create_interactive_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimization Methods Comparison\n",
    "\n",
    "Let's compare different optimization approaches on the same data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compare_optimization_methods(y_true, y_prob, metric='f1'):\n    \"\"\"Compare different threshold optimization methods.\"\"\"\n\n    print(f\"Comparing optimization methods for {metric.upper()} score...\\n\")\n\n    # Method 1: Smart brute force (our recommended approach)\n    result_brute = optimize_thresholds(y_true, y_prob, metric=metric, method='smart_brute')\n    thresh_brute = result_brute.threshold\n    score_brute = _metric_score(y_true, y_prob, thresh_brute, metric)\n\n    # Method 2: scipy.optimize.minimize_scalar (continuous optimization)\n    result = optimize.minimize_scalar(\n        lambda t: -_metric_score(y_true, y_prob, t, metric),\n        bounds=(0, 1),\n        method='bounded'\n    )\n    thresh_minimize = result.x\n    score_minimize = _metric_score(y_true, y_prob, thresh_minimize, metric)\n\n    # Method 3: With fallback (what our 'minimize' method actually does)\n    result_fallback = optimize_thresholds(y_true, y_prob, metric=metric, method='minimize')\n    thresh_fallback = result_fallback.threshold\n    score_fallback = _metric_score(y_true, y_prob, thresh_fallback, metric)\n\n    # Display results\n    methods = [\n        ('Smart Brute Force', thresh_brute, score_brute),\n        ('minimize_scalar Only', thresh_minimize, score_minimize),\n        ('With Fallback', thresh_fallback, score_fallback)\n    ]\n\n    for name, threshold, score in methods:\n        print(f\"{name:18} | Threshold: {threshold:.4f} | {metric.upper()}: {score:.4f}\")\n\n    # Create visualization\n    thresholds = np.linspace(0.01, 0.99, 500)\n    scores = [_metric_score(y_true, y_prob, t, metric) for t in thresholds]\n\n    unique_probs = np.unique(y_prob)\n    unique_scores = [_metric_score(y_true, y_prob, t, metric) for t in unique_probs]\n\n    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n\n    # Plot metric function\n    ax.plot(thresholds, scores, 'b-', linewidth=1.5, alpha=0.7, label=f'{metric.upper()} Score')\n\n    # Plot breakpoints\n    ax.scatter(unique_probs, unique_scores, color='lightcoral', s=30, alpha=0.6,\n              label=f'Breakpoints ({len(unique_probs)} points)')\n\n    # Plot results from different methods\n    colors = ['green', 'red', 'blue']\n    markers = ['*', 'x', 'D']\n\n    for (name, threshold, score), color, marker in zip(methods, colors, markers, strict=False):\n        ax.scatter([threshold], [score], color=color, s=120, marker=marker,\n                  zorder=5, label=f'{name}\\n(t={threshold:.3f})', edgecolors='black')\n\n    ax.set_xlabel('Decision Threshold')\n    ax.set_ylabel(f'{metric.upper()} Score')\n    ax.set_title('Comparison of Optimization Methods')\n    ax.grid(True, alpha=0.3)\n    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n\n    plt.tight_layout()\n    plt.show()\n\n    return methods\n\n# Test with example data\nnp.random.seed(123)\nn = 15\ny_test = np.random.randint(0, 2, n)\ny_prob_test = np.random.beta(2, 2, n)\n\nprint(f\"Test data: {n} samples with {len(np.unique(y_prob_test))} unique probabilities\\n\")\nresults = compare_optimization_methods(y_test, y_prob_test, 'f1')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Why the Fallback Mechanism Works\n",
    "\n",
    "The key insight is that **the optimal threshold must be one of the unique predicted probabilities**. Here's why:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def demonstrate_optimal_at_breakpoints():\n    \"\"\"Show that the optimal threshold is always at a breakpoint.\"\"\"\n\n    # Create example with clear optimal point\n    y_true = np.array([0, 0, 1, 1, 0, 1])\n    y_prob = np.array([0.2, 0.3, 0.6, 0.7, 0.8, 0.9])\n\n    print(\"Demonstrating that optimal threshold is at a breakpoint...\\n\")\n    print(f\"Data: labels = {y_true}\")\n    print(f\"      probs  = {y_prob}\\n\")\n\n    # Evaluate F1 at each unique probability\n    unique_probs = np.unique(y_prob)\n    print(\"F1 score at each unique probability (breakpoint):\")\n\n    for _i, prob in enumerate(unique_probs):\n        f1 = _metric_score(y_true, y_prob, prob, 'f1')\n        tp, tn, fp, fn = get_confusion_matrix(y_true, y_prob, prob)\n        print(f\"  t = {prob:.1f}: F1 = {f1:.3f} | TP={tp}, TN={tn}, FP={fp}, FN={fn}\")\n\n    # Find optimal\n    result = optimize_thresholds(y_true, y_prob, metric='f1')\n    optimal_thresh = result.threshold\n    optimal_f1 = _metric_score(y_true, y_prob, optimal_thresh, 'f1')\n\n    print(f\"\\nâ†’ Optimal: t = {optimal_thresh:.1f}, F1 = {optimal_f1:.3f}\")\n\n    # Now test a threshold between breakpoints\n    between_thresh = 0.65  # Between 0.6 and 0.7\n    between_f1 = _metric_score(y_true, y_prob, between_thresh, 'f1')\n\n    print(f\"\\nFor comparison, at t = {between_thresh:.2f} (between breakpoints):\")\n    print(f\"  F1 = {between_f1:.3f} (same as t = 0.6 because both give same predictions)\")\n\n    # Visualize predictions at different thresholds\n    print(\"\\nPrediction vectors:\")\n    for thresh in [0.6, 0.65, 0.7]:\n        predictions = (y_prob >= thresh).astype(int)\n        print(f\"  t = {thresh:.2f}: {predictions}\")\n\n    print(\"\\nâ†’ Note: t=0.6 and t=0.65 give the same predictions, hence same F1!\")\n\ndemonstrate_optimal_at_breakpoints()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multiple Metrics Comparison\n",
    "\n",
    "Different metrics often have different optimal thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compare_multiple_metrics(y_true, y_prob):\n    \"\"\"Show how different metrics have different optimal thresholds.\"\"\"\n\n    metrics = ['accuracy', 'f1', 'precision', 'recall']\n    colors = ['blue', 'red', 'green', 'orange']\n\n    thresholds = np.linspace(0.05, 0.95, 200)\n\n    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n\n    results = {}\n\n    for metric, color in zip(metrics, colors, strict=False):\n        # Calculate scores across threshold range\n        scores = [_metric_score(y_true, y_prob, t, metric) for t in thresholds]\n        ax.plot(thresholds, scores, color=color, linewidth=2, label=metric.capitalize())\n\n        # Find optimal threshold\n        result = optimize_thresholds(y_true, y_prob, metric=metric)\n        optimal_thresh = result.threshold\n        optimal_score = _metric_score(y_true, y_prob, optimal_thresh, metric)\n\n        # Mark optimal point\n        ax.scatter([optimal_thresh], [optimal_score], color=color, s=150,\n                  marker='*', zorder=5, edgecolors='black', linewidth=1)\n\n        results[metric] = (optimal_thresh, optimal_score)\n\n    # Add breakpoint lines\n    unique_probs = np.unique(y_prob)\n    for prob in unique_probs:\n        ax.axvline(x=prob, color='gray', linestyle='--', alpha=0.3)\n\n    ax.set_xlabel('Decision Threshold')\n    ax.set_ylabel('Metric Score')\n    ax.set_title('Different Metrics Have Different Optimal Thresholds\\n' +\n                '(Stars show optimal points, dashed lines show breakpoints)')\n    ax.grid(True, alpha=0.3)\n    ax.legend()\n    ax.set_ylim(0, 1.05)\n\n    plt.tight_layout()\n    plt.show()\n\n    # Print results\n    print(\"Optimal thresholds by metric:\")\n    for metric, (thresh, score) in results.items():\n        print(f\"  {metric:9}: t = {thresh:.3f}, score = {score:.3f}\")\n\n    return results\n\n# Demo with well-separated data\ny_demo = np.array([0, 0, 0, 1, 1, 1])\np_demo = np.array([0.1, 0.3, 0.4, 0.6, 0.8, 0.9])\n\nprint(f\"Demo data: labels = {y_demo}\")\nprint(f\"           probs  = {p_demo}\\n\")\n\nmetric_results = compare_multiple_metrics(y_demo, p_demo)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practical Implications\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Piecewise-Constant Nature**: Classification metrics only change at unique probability values\n",
    "\n",
    "2. **Optimization Challenge**: Continuous optimizers can get stuck in flat regions and miss the global optimum\n",
    "\n",
    "3. **Smart Solution**: Evaluate metrics at all unique probabilities (guaranteed global optimum)\n",
    "\n",
    "4. **Fallback Mechanism**: Combine continuous optimization with discrete evaluation for robustness\n",
    "\n",
    "5. **Metric Differences**: Different metrics often have different optimal thresholds\n",
    "\n",
    "### When This Matters Most\n",
    "\n",
    "- **Imbalanced datasets**: Default 0.5 threshold is often far from optimal\n",
    "- **Cost-sensitive decisions**: When false positives and false negatives have different costs\n",
    "- **Metric optimization**: When you need to maximize a specific metric (F1, precision, recall)\n",
    "- **Model deployment**: When converting probabilities to hard predictions\n",
    "\n",
    "### Computational Efficiency\n",
    "\n",
    "The smart brute force approach is actually very efficient:\n",
    "- **Time complexity**: O(k) where k = number of unique probabilities\n",
    "- **Typical case**: k â‰ª n (much fewer unique probabilities than samples)\n",
    "- **Guaranteed optimum**: No risk of local minima or convergence issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final demonstration: efficiency comparison\nimport time\n\n\ndef efficiency_demo():\n    \"\"\"Demonstrate the efficiency of smart brute force vs continuous optimization.\"\"\"\n\n    # Generate larger dataset\n    np.random.seed(42)\n    n_samples = 1000\n    y_large = np.random.randint(0, 2, n_samples)\n    p_large = np.random.beta(2, 2, n_samples)\n\n    n_unique = len(np.unique(p_large))\n\n    print(f\"Efficiency test with {n_samples} samples, {n_unique} unique probabilities\\n\")\n\n    methods = [\n        ('smart_brute', 'Smart Brute Force'),\n        ('minimize', 'Minimize with Fallback'),\n        ('gradient', 'Gradient Method')\n    ]\n\n    for method_code, method_name in methods:\n        start_time = time.time()\n        result = optimize_thresholds(y_large, p_large, metric='f1', method=method_code)\n        end_time = time.time()\n\n        threshold = result.threshold\n        score = _metric_score(y_large, p_large, threshold, 'f1')\n        duration = end_time - start_time\n\n        print(f\"{method_name:20} | Time: {duration:.4f}s | F1: {score:.4f} | Threshold: {threshold:.4f}\")\n\n    print(f\"\\nâ†’ Smart brute force evaluates only {n_unique} points vs {n_samples} samples!\")\n\nefficiency_demo()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ðŸŽ“ Conclusion\n\nThis notebook demonstrated the piecewise-constant nature of classification metrics and why this creates challenges for traditional optimization methods. The `optimal-classification-cutoffs` library addresses these challenges through:\n\n1. **Smart algorithms** that leverage the mathematical structure of the problem\n2. **Fallback mechanisms** that ensure robust optimization  \n3. **Efficient implementation** that scales well with dataset size\n\n## ðŸš€ What's Next?\n\nNow that you understand the mathematical foundations:\n\n- **Apply to your data**: Use the techniques from our examples\n- **01_quickstart.py**: Get immediate performance improvements\n- **02_business_value.py**: Optimize for real business metrics\n- **03_multiclass.py**: Handle complex multi-class scenarios\n\n## ðŸ“š Additional Resources\n\n- [Full Documentation](https://finite-sample.github.io/optimal-classification-cutoffs/)\n- [GitHub Repository](https://github.com/finite-sample/optimal-classification-cutoffs)\n- [Paper/Theory](https://finite-sample.github.io/optimal-classification-cutoffs/theory/)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}